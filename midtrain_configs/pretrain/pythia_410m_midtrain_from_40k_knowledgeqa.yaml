# config.yaml

model_name: pythia-410m

model_config:

out_dir: /projects/bfcu/mliu7/all_in_one_pretrainingpretrained_chkpts/pythia_410m_128b_fixed_midtrain_qa

precision: bf16-true

initial_checkpoint_dir:

resume: true

tokenizer_dir: /data/tir/projects/tir3/users/mengyan3/manifold_data/base_tokenizers/checkpoints/EleutherAI/pythia-1b/

data:
  class_path: litgpt.data.ShardedMixedDataset  
  init_args:
    data_path: /work/hdd/bfcu/mliu7/data/c4_merged/
    val_data_path: /work/hdd/bfcu/mliu7/data/c4_pythia_val/
    num_workers: 8
    literal_weights_str: "main/1:0.80,q4:0.15,q1:0.03,q2:0.015,q3:0.005"



# 61035 - 40000
train:
  max_iters: 10000000
  max_additional_steps: 1035
  micro_batch_size: 8
  global_batch_size: 1024
  log_interval: 10
  save_interval: 2000
  min_lr: 1e-6
  lr_scheduler: "cosine"
  lr_warmup_fraction: 0.1

eval:
  interval: 1000

optimizer:
  class_path: torch.optim.AdamW

  init_args:
    weight_decay: 0.1
    
    betas:
      - 0.9
      - 0.95

logger_name: wandb
